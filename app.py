# app.py â€“ å›½ç”»é…è‰²ç³»ç»Ÿ + è…¾è®¯æ··å…ƒ3D
from flask import Flask, request, jsonify, send_from_directory
from functools import wraps
from time import time
from collections import defaultdict
import joblib
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from scipy.optimize import nnls
import os
import sys
from typing import Dict, Any, Tuple
import base64
from PIL import Image
import io

# =============== è…¾è®¯äº‘SDKå¯¼å…¥===============
from tencentcloud.common import credential
from tencentcloud.common.profile.client_profile import ClientProfile
from tencentcloud.common.profile.http_profile import HttpProfile
from tencentcloud.ai3d.v20250513 import ai3d_client, models

# =============== é…ç½®å›½ç”»é…è‰²utilsè·¯å¾„ ===============
sys.path.append(os.path.join(os.path.dirname(__file__), 'scaler/py/utils'))
from scaler.py.utils.color_utils import rgb_to_ks, load_and_normalize_data

data_files = ['scaler/data/key.csv', 'scaler/data/data.csv']

# ==================== å›½ç”»é…è‰²å‚æ•°é…ç½® =====================================
# HSLåˆ¤å®šé˜ˆå€¼
HSL_WHITE_L_THRESHOLD = 0.85       
HSL_BLACK_L_THRESHOLD = 0.12       
GRAY_SAT_THRESHOLD = 0.15          

# è¡¥è‰²å¼€å…³
ADD_COMPONENT_ENABLE = True        
ADD_COMPONENT_RATIO = 0.08         

# èåˆå‚æ•°
LOOKUP_THRESHOLD = 8.0             
FUSION_TOP_K = 5                   
FUSION_POWER = 2                   
FUSION_CONFIDENCE = 1.0           

# å­˜åœ¨æ€§åˆ¤æ–­é˜ˆå€¼
PRESENCE_THRESHOLD = 0.015         
MIN_RETAIN_COLORS = 2              

# HybridéªŒè¯é˜ˆå€¼
HYBRID_ERROR_THRESHOLD = 30.0      

# ä¸­æ–‡æ˜ å°„
CHINESE_NAME_MAP = {
    "white": "ç™½è‰²",
    "stoneGreen": "çŸ³ç»¿",
    "stoneBlue": "çŸ³é’", 
    "vineYellow": "è—¤é»„",
    "red": "æœ±çº¢",
    "ocher": "èµ­çŸ³",
    "black": "é»‘è‰²"
}

# =============== æ··å…ƒ3Dé…ç½®===============
TENCENT_SECRET_ID = 
TENCENT_SECRET_KEY =      
TENCENT_REGION = "ap-guangzhou"
# ====================================================================

# =============== é˜²ç©º ===============
MAX_IMAGE_SIZE = 5 * 1024 * 1024  # 5MB
ALLOWED_IMAGE_TYPES = {'.jpg', '.jpeg', '.png', '.webp', '.bmp'}
MAX_IMAGE_DIMENSION = 2048  # æœ€å¤§è¾¹é•¿
MAX_PROMPT_LENGTH = 500  # æ–‡å­—æè¿°æœ€å¤§é•¿åº¦
RATE_LIMIT_PER_MIN = 10  # æ¯åˆ†é’Ÿæœ€å¤§è¯·æ±‚æ•°
# å†…å­˜é¢‘ç‡é™åˆ¶å™¨
rate_limit_store = defaultdict(list)
# ====================================================================

# æ¨¡å‹è·¯å¾„
KM_IMPROVED_PATH = r'./models/km_correct.pkl'
HYBRID_MODEL_PATH = r'./models/hybrid_model.pkl'
INVERSE_MODEL_PATH = r'./models/inverse_transformer.pkl'

# å…¨å±€å˜é‡
km_improved_model = None
hybrid_model = None
inverse_model = None
km_A_matrix = None
PIGMENT_NAMES_EN = []
PIGMENT_NAMES_CN = []
WHITE_IDX = -1
BLACK_IDX = -1
rgb_lookup_table = {}

class InverseTransformer(nn.Module):
    """é€†å‘Transformerï¼šRGB â†’ Recipe"""
    def __init__(self, d_model: int = 128, nhead: int = 8, num_layers: int = 4):
        super().__init__()
        self.rgb_encoder = nn.Sequential(
            nn.Linear(3, d_model), nn.ReLU(), nn.Linear(d_model, d_model)
        )
        self.pos_encoding = nn.Parameter(torch.randn(1, 31, d_model) * 0.1)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=nhead, dim_feedforward=d_model*4,
            dropout=0.1, batch_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)
        
        self.recipe_decoder = nn.Sequential(
            nn.Linear(d_model * 31, 256), nn.ReLU(), nn.Dropout(0.1),
            nn.Linear(256, 128), nn.ReLU(), nn.Linear(128, 7), nn.Sigmoid()
        )
        
    def forward(self, rgb: torch.Tensor) -> torch.Tensor:
        x = self.rgb_encoder(rgb.unsqueeze(1))
        x = x.expand(-1, 31, -1)
        x = x + self.pos_encoding
        x = self.encoder(x)
        x = x.reshape(x.size(0), -1)
        recipe = self.recipe_decoder(x)
        return recipe

class HybridModel(nn.Module):
    """Hybridæ¨¡å‹ï¼šRecipe â†’ RGB + K/Sï¼ˆåŠ¨æ€ç»´åº¦ï¼‰"""
    def __init__(self, km_A_matrix, n_pigments=7, d_model=None):
        super().__init__()
        self.register_buffer('A_matrix', torch.tensor(km_A_matrix, dtype=torch.float32))
        self.n_pigments = n_pigments
        
        if d_model:
            dim1 = d_model
            dim2 = d_model * 2
        else:
            dim1 = 128
            dim2 = 256
        
        self.transformer = nn.Sequential(
            nn.Linear(n_pigments, dim1), nn.ReLU(),
            nn.Linear(dim1, dim2), nn.ReLU(),
            nn.Linear(dim2, 31)
        )
        
        dim_rgb = 64
        self.rgb_head = nn.Sequential(
            nn.Linear(31, dim_rgb), nn.ReLU(), nn.Linear(dim_rgb, 3), nn.Sigmoid()
        )
        
    def forward(self, recipe):
        ks_km = recipe @ self.A_matrix.T
        ks_residual = self.transformer(recipe)
        ks = ks_km + ks_residual
        rgb = self.rgb_head(ks)
        return rgb, ks

def load_models():
    """åŠ è½½ä¸‰ä¸ªæ¨¡å‹ï¼ˆåŠ¨æ€ç»´åº¦ç‰ˆï¼‰"""
    global km_improved_model, hybrid_model, inverse_model, km_A_matrix
    global PIGMENT_NAMES_EN, PIGMENT_NAMES_CN, WHITE_IDX, BLACK_IDX
    
    try:
        if not os.path.exists(KM_IMPROVED_PATH):
            raise FileNotFoundError(f"KMæ”¹è¿›æ¨¡å‹ä¸å­˜åœ¨: {KM_IMPROVED_PATH}")
        
        km_data = joblib.load(KM_IMPROVED_PATH)
        km_A_matrix = km_data['A_matrix']
        
        if 'pigment_names' in km_data:
            PIGMENT_NAMES_EN = km_data['pigment_names']
        else:
            df_temp = load_and_normalize_data(['scaler/data/key.csv'])
            PIGMENT_NAMES_EN = df_temp.columns[3:].tolist()
        
        PIGMENT_NAMES_CN = [CHINESE_NAME_MAP.get(name, name) for name in PIGMENT_NAMES_EN]
        print(f"âœ… KMæ”¹è¿›æ¨¡å‹åŠ è½½æˆåŠŸ: A_MATRIX {km_A_matrix.shape}")
        
        if not os.path.exists(HYBRID_MODEL_PATH):
            raise FileNotFoundError(f"Hybridæ¨¡å‹ä¸å­˜åœ¨: {HYBRID_MODEL_PATH}")
        
        with torch.serialization.safe_globals([np._core.multiarray._reconstruct]):
            hybrid_checkpoint = torch.load(HYBRID_MODEL_PATH, map_location='cpu', weights_only=False)
        
        state_dict = hybrid_checkpoint['model_state']
        dim1 = state_dict['transformer.0.weight'].shape[0]
        dim2 = state_dict['transformer.2.weight'].shape[0]
        
        print(f"   Hybridæ¨¡å‹ç»´åº¦: {dim1}â†’{dim2}â†’31ï¼ˆä»checkpointæ¨æ–­ï¼‰")
        
        hybrid_model = HybridModel(
            hybrid_checkpoint['km_A_matrix'], 
            n_pigments=len(PIGMENT_NAMES_EN),
            d_model=dim1
        )
        hybrid_model.load_state_dict(state_dict)
        hybrid_model.eval()
        print(f"âœ… Hybridæ¨¡å‹åŠ è½½æˆåŠŸ")
        
        if not os.path.exists(INVERSE_MODEL_PATH):
            raise FileNotFoundError(f"é€†å‘æ¨¡å‹ä¸å­˜åœ¨: {INVERSE_MODEL_PATH}")
        
        inverse_checkpoint = torch.load(INVERSE_MODEL_PATH, map_location='cpu')
        model_config = inverse_checkpoint.get('model_config', {})
        
        inverse_model = InverseTransformer(
            d_model=model_config.get('d_model', 128),
            nhead=model_config.get('nhead', 8),
            num_layers=model_config.get('num_layers', 4)
        )
        inverse_model.load_state_dict(inverse_checkpoint['model_state'])
        inverse_model.eval()
        
        print(f"âœ… é€†å‘TransformeråŠ è½½æˆåŠŸ: {len(PIGMENT_NAMES_EN)} ç§é¢œæ–™")
        print(f"   æ˜ å°„å…³ç³»: {dict(zip(PIGMENT_NAMES_EN, PIGMENT_NAMES_CN))}")
        
        WHITE_IDX = PIGMENT_NAMES_EN.index('white') if 'white' in PIGMENT_NAMES_EN else -1
        BLACK_IDX = PIGMENT_NAMES_EN.index('black') if 'black' in PIGMENT_NAMES_EN else -1
        
        print(f"   ç™½è‰²ç´¢å¼•: {WHITE_IDX}ï¼ˆ{PIGMENT_NAMES_CN[WHITE_IDX] if WHITE_IDX>=0 else 'æ— '}ï¼‰")
        print(f"   é»‘è‰²ç´¢å¼•: {BLACK_IDX}ï¼ˆ{PIGMENT_NAMES_CN[BLACK_IDX] if BLACK_IDX>=0 else 'æ— '}ï¼‰")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        exit(1)

def load_lookup_table():
    """åŠ è½½RGBæŸ¥æ‰¾è¡¨"""
    global rgb_lookup_table
    
    try:
        df_all = load_and_normalize_data(data_files)
        for _, row in df_all.iterrows():
            rgb_key = (int(row['R']), int(row['G']), int(row['B']))
            rgb_lookup_table[rgb_key] = row[PIGMENT_NAMES_EN].values.astype(np.float64)
        print(f"âœ… RGBæŸ¥æ‰¾è¡¨æ„å»ºå®Œæˆ: {len(rgb_lookup_table)} æ¡è®°å½•")
    except Exception as e:
        print(f"âš ï¸  æ•°æ®é›†åŠ è½½è­¦å‘Š: {e}")

def rgb_to_hsl(rgb: np.ndarray) -> np.ndarray:
    """RGBè½¬HSL"""
    r, g, b = rgb / 255.0
    max_, min_ = max(r, g, b), min(r, g, b)
    delta = max_ - min_
    l = (max_ + min_) / 2
    
    if delta == 0:
        s, h = 0, 0
    else:
        s = delta / (1 - abs(2 * l - 1))
        if max_ == r: h = 60 * (((g - b) / delta) % 6)
        elif max_ == g: h = 60 * ((b - r) / delta + 2)
        else: h = 60 * ((r - g) / delta + 4)
    
    return np.array([h, s, l])

def ultra_safe_normalize(w: np.ndarray) -> np.ndarray:
    """å½’ä¸€åŒ–"""
    w = np.nan_to_num(np.clip(np.asarray(w, dtype=np.float64), 0, 1), 
                      nan=0.0, posinf=1.0, neginf=0.0)
    return w / max(w.sum(), 1e-6)

def rgb_distance(rgb1: np.ndarray, rgb2: np.ndarray) -> float:
    return np.linalg.norm(rgb1 - rgb2)

def rgb_fusion_predict(target_rgb: np.ndarray) -> Tuple[Dict[str, float], float]:
    """RGBèåˆé¢„æµ‹ï¼ˆè¿”å›ä¸­æ–‡ç»“æœï¼‰"""
    target_rgb = np.asarray(target_rgb, dtype=np.int32)
    
    if not rgb_lookup_table:
        return {}, 0.0
    
    dist_list = [(rgb_distance(target_rgb, k), v) for k, v in rgb_lookup_table.items()]
    dist_list.sort(key=lambda x: x[0])
    
    if not dist_list:
        return {}, 0.0
    
    topk = dist_list[:FUSION_TOP_K]
    d_max = topk[-1][0] + 1e-6
    
    weights = np.array([(d_max - d[0]) / d_max ** FUSION_POWER for d in topk])
    weights = weights / (weights.sum() + 1e-6)
    
    recipe = np.average([d[1] for d in topk], axis=0, weights=weights)
    recipe = ultra_safe_normalize(recipe)
    
    result = {
        PIGMENT_NAMES_CN[i]: round(recipe[i] * 100, 1)
        for i in range(len(PIGMENT_NAMES_EN)) if recipe[i] > 0.01
    }
    
    confidence = float(np.max(recipe)) if len(recipe) > 0 else 0.0
    return result, confidence

def get_presence_mask_from_km(rgb: np.ndarray) -> np.ndarray:
    """ä½¿ç”¨KMæ”¹è¿›æ¨¡å‹åˆ¤æ–­é¢œè‰²å­˜åœ¨æ€§"""
    ks = rgb_to_ks(rgb, wavelengths=np.arange(400, 710, 10), rgb_wavelengths=np.array([630, 530, 450]))[0]
    
    w_km, _ = nnls(km_A_matrix, ks)
    w_km_norm = ultra_safe_normalize(w_km)
    
    presence_mask = w_km_norm > PRESENCE_THRESHOLD
    
    if presence_mask.sum() < MIN_RETAIN_COLORS:
        top_k_idx = np.argsort(w_km_norm)[-MIN_RETAIN_COLORS:]
        presence_mask = np.zeros_like(w_km_norm, dtype=bool)
        presence_mask[top_k_idx] = True
    
    existing = [(PIGMENT_NAMES_CN[i], w_km_norm[i]) for i in range(len(w_km_norm)) if presence_mask[i]]
    print(f"   KMå­˜åœ¨æ€§åˆ¤æ–­: ä¿ç•™ {[f'{name}({val:.3f})' for name, val in existing]}")
    
    removed = [(PIGMENT_NAMES_CN[i], w_km_norm[i]) for i in range(len(w_km_norm)) if not presence_mask[i]]
    if removed:
        print(f"   KMè¿‡æ»¤é¢œè‰²: {[(name, f'{val:.3f}') for name, val in removed]}")
        print("-"*60)
    return presence_mask

def predict_with_km_fallback(rgb: np.ndarray, hsl: np.ndarray) -> np.ndarray:
    """KMæ¨¡å‹å¤‡ç”¨æ–¹æ¡ˆ"""
    ks = rgb_to_ks(rgb, wavelengths=np.arange(400, 710, 10), rgb_wavelengths=np.array([630, 530, 450]))[0]
    
    w_raw, _ = nnls(km_A_matrix, ks)
    w_norm = ultra_safe_normalize(w_raw)
    
    existing_mask = w_norm > PRESENCE_THRESHOLD
    if existing_mask.sum() < MIN_RETAIN_COLORS:
        top_k_idx = np.argsort(w_norm)[-MIN_RETAIN_COLORS:]
        existing_mask = np.zeros_like(w_norm, dtype=bool)
        existing_mask[top_k_idx] = True
    
    w_filtered = w_norm * existing_mask
    w_norm = ultra_safe_normalize(w_filtered)
    
    if WHITE_IDX >= 0 and BLACK_IDX >= 0:
        if hsl[2] > HSL_WHITE_L_THRESHOLD and hsl[1] < GRAY_SAT_THRESHOLD:
            if w_norm[BLACK_IDX] > 0.01:
                w_norm[BLACK_IDX] = 0.0
                w_norm = ultra_safe_normalize(w_norm)
            if ADD_COMPONENT_ENABLE and w_norm[WHITE_IDX] < 0.01:
                add_white = max(0.05, (hsl[2] - HSL_WHITE_L_THRESHOLD) * ADD_COMPONENT_RATIO)
                w_norm[WHITE_IDX] = add_white
                w_norm = ultra_safe_normalize(w_norm)
        elif hsl[2] < HSL_BLACK_L_THRESHOLD:
            if w_norm[WHITE_IDX] > 0.01:
                w_norm[WHITE_IDX] = 0.0
                w_norm = ultra_safe_normalize(w_norm)
            if ADD_COMPONENT_ENABLE and w_norm[BLACK_IDX] < 0.01:
                add_black = max(0.05, (HSL_BLACK_L_THRESHOLD - hsl[2]) * ADD_COMPONENT_RATIO)
                w_norm[BLACK_IDX] = add_black
                w_norm = ultra_safe_normalize(w_norm)
    
    return w_norm

def predict_with_inverse_model(rgb: np.ndarray, hsl: np.ndarray, 
                               enable_fallback: bool = True) -> np.ndarray:
    """é€†å‘æ¨¡å‹é¢„æµ‹ + KMå­˜åœ¨æ€§åˆ¤æ–­ + HybridéªŒè¯"""
    try:
        presence_mask = get_presence_mask_from_km(rgb)
        
        with torch.no_grad():
            rgb_tensor = torch.tensor(rgb / 255.0, dtype=torch.float32).unsqueeze(0)
            w_raw = inverse_model(rgb_tensor).squeeze().numpy()
        
        print(f"   åŸå§‹é¢„æµ‹: {[f'{PIGMENT_NAMES_CN[i]}:{w_raw[i]:.3f}' for i in range(len(w_raw))]}")
        
        w_filtered = w_raw * presence_mask
        filtered_info = [(PIGMENT_NAMES_CN[i], w_filtered[i]) for i in range(len(w_filtered)) if w_filtered[i] > 0]
        print(f"   åº”ç”¨KMæ©ç : {filtered_info}")
        
        if w_filtered.sum() > 0:
            w_norm = ultra_safe_normalize(w_filtered)
        else:
            print(f"âš ï¸ è¿‡æ»¤åå…¨ä¸ºé›¶ï¼Œå›é€€åˆ°KMé…æ–¹")
            return predict_with_km_fallback(rgb, hsl)
        
        if WHITE_IDX >= 0 and BLACK_IDX >= 0:
            is_bright_gray = hsl[2] > HSL_WHITE_L_THRESHOLD and hsl[1] < GRAY_SAT_THRESHOLD
            is_dark = hsl[2] < HSL_BLACK_L_THRESHOLD
            
            if is_bright_gray:
                if w_norm[BLACK_IDX] > 0.01:
                    print(f"âš ï¸ é«˜äº®ç°è‰²(L={hsl[2]:.3f}) åˆ é™¤å†²çªè‰² {PIGMENT_NAMES_CN[BLACK_IDX]} {w_norm[BLACK_IDX]*100:.1f}%")
                    w_norm[BLACK_IDX] = 0.0
                    w_norm = ultra_safe_normalize(w_norm)
                
                if ADD_COMPONENT_ENABLE and w_norm[WHITE_IDX] < 0.01:
                    add_white = max(0.05, (hsl[2] - HSL_WHITE_L_THRESHOLD) * ADD_COMPONENT_RATIO)
                    w_norm[WHITE_IDX] = add_white
                    w_norm = ultra_safe_normalize(w_norm)
                    print(f"âœ… åç™½è¡¥è‰² {PIGMENT_NAMES_CN[WHITE_IDX]} {add_white*100:.1f}%")
            
            elif is_dark:
                if w_norm[WHITE_IDX] > 0.01:
                    print(f"âš ï¸ ä½äº®(L={hsl[2]:.3f}) åˆ é™¤å†²çªè‰² {PIGMENT_NAMES_CN[WHITE_IDX]} {w_norm[WHITE_IDX]*100:.1f}%")
                    w_norm[WHITE_IDX] = 0.0
                    w_norm = ultra_safe_normalize(w_norm)
                
                if ADD_COMPONENT_ENABLE and w_norm[BLACK_IDX] < 0.01:
                    add_black = max(0.05, (HSL_BLACK_L_THRESHOLD - hsl[2]) * ADD_COMPONENT_RATIO)
                    w_norm[BLACK_IDX] = add_black
                    w_norm = ultra_safe_normalize(w_norm)
                    print(f"âœ… åé»‘è¡¥è‰² {PIGMENT_NAMES_CN[BLACK_IDX]} {add_black*100:.1f}%")
        
        print("-"*60)
        print("   è¿›è¡ŒHybridæ¨¡å‹éªŒè¯...")
        with torch.no_grad():
            w_tensor = torch.tensor(w_norm, dtype=torch.float32).unsqueeze(0)
            pred_rgb, _ = hybrid_model(w_tensor)
            pred_rgb_np = (pred_rgb.squeeze().numpy() * 255).astype(np.int32)
        
        error = np.linalg.norm(pred_rgb_np - rgb)
        print(f"   HybridéªŒè¯: é¢„æµ‹RGB={pred_rgb_np.tolist()}, ç›®æ ‡RGB={rgb.astype(int).tolist()}, è¯¯å·®={error:.2f}")
        
        if error > HYBRID_ERROR_THRESHOLD:
            print(f"âŒ HybridéªŒè¯å¤±è´¥: è¯¯å·®è¿‡å¤§({error:.2f}>{HYBRID_ERROR_THRESHOLD:.2f})")
            print(rgb,hsl)
        else:
            print(f"âœ… HybridéªŒè¯é€šè¿‡")
        
        return w_norm
        
    except Exception as e:
        print(f"âŒ é€†å‘æ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
        if enable_fallback:
            print(f"   å›é€€åˆ°KMæ¨¡å‹...")
            return predict_with_km_fallback(rgb, hsl)
        else:
            raise

## =============== é˜²ç©ºè®¾è®¡å·¥å…·å‡½æ•° ===============
def validate_and_clean_image(image_data: str) -> Tuple[str, Dict[str, Any]]:
    """é˜²ç©ºæ ¸å¿ƒï¼šéªŒè¯å›¾ç‰‡å¹¶è¿”å›æ¸…ç†åçš„æ•°æ®"""
    if not image_data:
        return "", {"valid": False, "error": "å›¾ç‰‡æ•°æ®ä¸ºç©º"}
    
    try:
        # é˜²ç©ºï¼šæ£€æŸ¥Base64æ•°æ®å¤´
        if not image_data.startswith('data:image/'):
            return "", {"valid": False, "error": "æ— æ•ˆçš„å›¾ç‰‡æ ¼å¼ï¼Œå¿…é¡»æ˜¯data URIæ ¼å¼"}
        
        # é˜²ç©ºï¼šæå–å®é™…æ•°æ®
        try:
            header, data = image_data.split(',', 1)
        except ValueError:
            return "", {"valid": False, "error": "å›¾ç‰‡æ•°æ®æ ¼å¼é”™è¯¯"}
        
        # é˜²ç©ºï¼šæ£€æŸ¥æ–‡ä»¶ç±»å‹
        file_type = header.split(';')[0].split('/')[1].lower()
        if f'.{file_type}' not in ALLOWED_IMAGE_TYPES:
            return "", {"valid": False, "error": f"ä¸æ”¯æŒçš„å›¾ç‰‡ç±»å‹: {file_type}ï¼Œä»…æ”¯æŒ{ALLOWED_IMAGE_TYPES}"}
        
        # é˜²ç©ºï¼šè§£ç å¹¶æ£€æŸ¥å¤§å°
        try:
            img_bytes = base64.b64decode(data)
        except Exception:
            return "", {"valid": False, "error": "Base64è§£ç å¤±è´¥"}
        
        if len(img_bytes) > MAX_IMAGE_SIZE:
            return "", {"valid": False, "error": f"å›¾ç‰‡å¤§å°è¶…è¿‡é™åˆ¶ï¼ˆ{MAX_IMAGE_SIZE / 1024 / 1024}MBï¼‰"}
        
        # é˜²ç©ºï¼šæ£€æŸ¥å›¾ç‰‡çœŸå®å°ºå¯¸å’Œæ ¼å¼
        try:
            with Image.open(io.BytesIO(img_bytes)) as img:
                width, height = img.size
                if width > MAX_IMAGE_DIMENSION or height > MAX_IMAGE_DIMENSION:
                    return "", {"valid": False, "error": f"å›¾ç‰‡å°ºå¯¸è¿‡å¤§: {width}x{height}ï¼ˆæœ€å¤§æ”¯æŒ{MAX_IMAGE_DIMENSION}pxï¼‰"}
                
                # é˜²ç©ºï¼šè½¬æ¢ä¸ºRGBæ¨¡å¼ï¼ˆé¿å…é€æ˜åº¦é—®é¢˜ï¼‰
                if img.mode in ('RGBA', 'LA', 'P', 'CMYK'):
                    img = img.convert('RGB')
                    # é‡æ–°è½¬æ¢ä¸ºBase64
                    buffered = io.BytesIO()
                    img.save(buffered, format='JPEG', quality=95)
                    clean_data = base64.b64encode(buffered.getvalue()).decode()
                    return f"data:image/jpeg;base64,{clean_data}", {"valid": True}
        except Exception as e:
            return "", {"valid": False, "error": f"å›¾ç‰‡è§£æå¤±è´¥: {str(e)}"}
        
        return image_data, {"valid": True}
        
    except Exception as e:
        return "", {"valid": False, "error": f"å›¾ç‰‡å¤„ç†å¤±è´¥: {str(e)}"}

def cleanup_temp_data(request_data: Dict[str, Any]):
    """é˜²ç©ºï¼šæ¸…ç†ä¸´æ—¶æ•°æ®é˜²æ­¢å†…å­˜æ³„æ¼"""
    # åˆ é™¤requestä¸­çš„å¤§å¯¹è±¡å¼•ç”¨
    if 'image' in request_data and len(request_data['image']) > 10000:
        request_data['image'] = '[IMAGE_DATA_CLEARED]'

def rate_limit(max_per_minute=RATE_LIMIT_PER_MIN):
    """é˜²ç©ºï¼šé¢‘ç‡é™åˆ¶è£…é¥°å™¨"""
    def decorator(f):
        @wraps(f)
        def wrapped(*args, **kwargs):
            client_ip = request.remote_addr
            now = time()
            
            # é˜²ç©ºï¼šæ¸…ç†è¿‡æœŸè®°å½•
            rate_limit_store[client_ip] = [
                t for t in rate_limit_store[client_ip] 
                if now - t < 60
            ]
            
            # é˜²ç©ºï¼šæ£€æŸ¥é¢‘ç‡å¹¶è¿”å›æ ‡å‡†åŒ–é”™è¯¯ç 
            if len(rate_limit_store[client_ip]) >= max_per_minute:
                return jsonify({
                    "success": False,
                    "error": f"è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œæ¯åˆ†é’Ÿæœ€å¤š{max_per_minute}æ¬¡",
                    "code": "RATE_LIMITED"
                }), 429
            
            rate_limit_store[client_ip].append(now)
            return f(*args, **kwargs)
        return wrapped
    return decorator

# =============== Flaskè·¯ç”± ===============
app = Flask(__name__, static_folder='.', static_url_path='')

@app.route('/')
def index():
    return send_from_directory('.', 'index.html')

@app.route('/predict', methods=['POST'])
def predict():
    """å›½ç”»é…è‰²é¢„æµ‹æ¥å£"""
    try:
        data = request.get_json()
        if not data or 'R' not in data or 'G' not in data or 'B' not in data:
            return jsonify({"error": "ç¼ºå°‘RGBå‚æ•°"}), 400
        
        rgb = np.array([data['R'], data['G'], data['B']], dtype=np.float64)
        hsl = rgb_to_hsl(rgb)
        enable_fallback = data.get('enableFallback', True)
        
        print("\n" + "="*60)
        print("ğŸ¯ æ–°é¢„æµ‹è¯·æ±‚")
        print(f"   RGB: {rgb.astype(int).tolist()}  HSL: H={hsl[0]:.1f}Â° S={hsl[1]:.3f} L={hsl[2]:.3f}")
        print(f"hsl: {hsl}")
        
        if hsl[2] > HSL_WHITE_L_THRESHOLD and hsl[1] < GRAY_SAT_THRESHOLD:
            print("ğŸª„ åˆ¤å®šä¸ºçº¯ç™½")
            white_name = PIGMENT_NAMES_CN[WHITE_IDX] if WHITE_IDX >= 0 else "ç™½è‰²"
            return jsonify({white_name: 100.0})
        
        if hsl[2] < HSL_BLACK_L_THRESHOLD:
            print("ğŸª„ åˆ¤å®šä¸ºçº¯é»‘")
            black_name = PIGMENT_NAMES_CN[BLACK_IDX] if BLACK_IDX >= 0 else "é»‘è‰²"
            return jsonify({black_name: 100.0})
        
        fusion_result, confidence = rgb_fusion_predict(rgb)
        print(f"èåˆæ–¹æ¡ˆ:{fusion_result}")
        print(f"[RGBèåˆ] ç½®ä¿¡åº¦={confidence:.3f}, é˜ˆå€¼={FUSION_CONFIDENCE}")
        
        if confidence >= FUSION_CONFIDENCE:
            print(f"âœ… é‡‡ç”¨èåˆé…æ–¹: {fusion_result}")
            return jsonify(fusion_result)
        
        rgb_key = tuple(rgb.astype(int))
        if rgb_key in rgb_lookup_table:
            print(f"âœ… æ•°æ®é›†ç²¾ç¡®å‘½ä¸­")
            w_norm = ultra_safe_normalize(rgb_lookup_table[rgb_key])
        else:
            print(f"âŒ æœªå‘½ä¸­æ•°æ®é›†ï¼Œå¯åŠ¨KM+Hybridæµç¨‹...")
            w_norm = predict_with_inverse_model(rgb, hsl, enable_fallback)
        
        result = {
            PIGMENT_NAMES_CN[i]: round(w_norm[i] * 100, 1)
            for i in range(len(PIGMENT_NAMES_EN)) if w_norm[i] > 0.005
        }
        
        print(f"ğŸ“¤ æœ€ç»ˆé…æ–¹: {result}")
        print("="*60)
        return jsonify(result)
        
    except Exception as e:
        print(f"âŒ é¢„æµ‹å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": f"é¢„æµ‹å¤±è´¥: {str(e)}"}), 500

# =============== æ··å…ƒ3Dæ¥å£===============
def create_ai3d_client():
    """åˆ›å»ºè…¾è®¯äº‘3Då®¢æˆ·ç«¯"""
    cred = credential.Credential(TENCENT_SECRET_ID, TENCENT_SECRET_KEY)
    client_profile = ClientProfile()
    client_profile.httpProfile = HttpProfile(endpoint="ai3d.tencentcloudapi.com")
    return ai3d_client.Ai3dClient(cred, TENCENT_REGION, client_profile)

@app.route('/api/hunyuan3d/submit', methods=['POST'])
@rate_limit(max_per_minute=RATE_LIMIT_PER_MIN)  # é˜²ç©ºï¼šé¢‘ç‡é™åˆ¶
def hunyuan3d_submit():
    """æäº¤æ··å…ƒ3Dä¸“ä¸šç‰ˆä»»åŠ¡ï¼ˆé˜²ç©ºå¼ºåŒ–ç‰ˆï¼‰"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                "success": False,
                "error": "è¯·æ±‚æ•°æ®ä¸èƒ½ä¸ºç©º",
                "code": "EMPTY_REQUEST"
            }), 400
        
        # é˜²ç©ºï¼šæå–å‚æ•°ï¼ˆå…è®¸ç©ºå­—ç¬¦ä¸²ï¼‰
        prompt = data.get("prompt", "").strip()
        image_data = data.get("image", "").strip()
        
        # é˜²ç©ºï¼šè‡³å°‘æä¾›ä¸€ä¸ªè¾“å…¥
        if not prompt and not image_data:
            return jsonify({
                "success": False,
                "error": "è¯·æä¾›æ–‡å­—æè¿°æˆ–å›¾ç‰‡", 
                "code": "NO_INPUT_PROVIDED"
            }), 400
        
        # é˜²ç©ºï¼šæ–‡å­—é•¿åº¦é™åˆ¶
        if len(prompt) > MAX_PROMPT_LENGTH:
            return jsonify({
                "success": False,
                "error": f"æ–‡å­—æè¿°è¿‡é•¿ï¼ˆæœ€å¤§{MAX_PROMPT_LENGTH}å­—ï¼‰", 
                "code": "PROMPT_TOO_LONG"
            }), 400
        
        # é˜²ç©ºï¼šå›¾ç‰‡éªŒè¯ï¼ˆå¦‚æœæœ‰ï¼‰
        clean_image = ""
        input_type = "text_only"  # è®°å½•è¾“å…¥ç±»å‹ç”¨äºç›‘æ§
        if image_data:
            clean_image, validation = validate_and_clean_image(image_data)
            if not validation["valid"]:
                return jsonify({
                    "success": False,
                    "error": validation["error"], 
                    "code": "INVALID_IMAGE"
                }), 400
            input_type = "image_only" if not prompt else "text+image"
        
        # é˜²ç©ºï¼šæ— æ–‡å­—æ—¶çš„æ—¥å¿—è®°å½•
        if not prompt and clean_image:
            print(f"âš ï¸ é˜²ç©ºè­¦å‘Š: æ”¶åˆ°çº¯å›¾ç‰‡è¯·æ±‚ï¼Œå»ºè®®è¡¥å……æ–‡å­—æè¿°ä»¥æå‡æ•ˆæœ")
        
        client = create_ai3d_client()
        req = models.SubmitHunyuanTo3DProJobRequest()
        
        # âœ… v20250513ä¸“ä¸šç‰ˆå‚æ•°
        params = {}
        if prompt:
            params["Prompt"] = prompt
        
        # é˜²ç©ºï¼šç¡®ä¿å›¾ç‰‡æ•°æ®æœ‰æ•ˆæ‰æ·»åŠ 
        if clean_image:
            params["Image"] = clean_image
        
        # é˜²ç©ºï¼šæ¸…ç†è¯·æ±‚æ•°æ®
        cleanup_temp_data(data)
        
        req.from_json_string(str(params).replace("'", '"'))
        resp = client.SubmitHunyuanTo3DProJob(req)
        
        return jsonify({
            "success": True,
            "jobId": resp.JobId,
            "requestId": resp.RequestId,
            "message": "ä»»åŠ¡æäº¤æˆåŠŸ",
            "inputType": input_type  # é˜²ç©ºï¼šè¿”å›è¾“å…¥ç±»å‹ä¾›å‰ç«¯å‚è€ƒ
        })

    except Exception as e:
        error_msg = f"æäº¤å¤±è´¥: {str(e)}"
        print(f"âŒ é˜²ç©ºæ•è·å¼‚å¸¸: {error_msg}")
        
        # é˜²ç©ºï¼šå¼‚å¸¸åˆ†ç±»
        error_code = "SUBMIT_ERROR"
        if "NoPermission" in str(e):
            error_code = "AUTH_FAILED"
        elif "LimitExceeded" in str(e):
            error_code = "RATE_LIMIT"
        
        return jsonify({
            "success": False,
            "error": error_msg,
            "code": error_code,
            "suggestion": "è¯·æ£€æŸ¥å›¾ç‰‡æ ¼å¼å’Œå¤§å°ï¼Œæˆ–ç¨åé‡è¯•"
        }), 500
        
    finally:
        # é˜²ç©ºï¼šå¼ºåˆ¶æ¸…ç†
        if 'data' in locals():
            cleanup_temp_data(data)

@app.route('/api/hunyuan3d/query/<job_id>')
def hunyuan3d_query(job_id):
    """æŸ¥è¯¢æ··å…ƒ3Dä¸“ä¸šç‰ˆä»»åŠ¡çŠ¶æ€"""
    try:
        if not job_id:
            return jsonify({"error": "JobIdä¸èƒ½ä¸ºç©º", "code": "INVALID_JOB_ID"}), 400

        client = create_ai3d_client()
        req = models.QueryHunyuanTo3DProJobRequest()
        req.JobId = job_id
        
        resp = client.QueryHunyuanTo3DProJob(req)
        
        result_files = []
        if resp.ResultFile3Ds:
            for file in resp.ResultFile3Ds:
                result_files.append({
                    "type": file.Type,
                    "url": file.Url,
                    "previewUrl": file.PreviewImageUrl
                })
        
        return jsonify({
            "success": True,
            "status": resp.Status,
            "errorCode": getattr(resp, "ErrorCode", ""),
            "errorMessage": getattr(resp, "ErrorMessage", ""),
            "resultFiles": result_files,
            "createTime": getattr(resp, "CreateTime", ""),
            "updateTime": getattr(resp, "UpdateTime", "")
        })

    except Exception as e:
        print(f"âŒ æŸ¥è¯¢ä»»åŠ¡å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({
            "success": False,
            "error": f"æŸ¥è¯¢å¤±è´¥: {str(e)}",
            "code": "QUERY_ERROR"
        }), 500

@app.route('/hunyuan3d')
def hunyuan3d_page():
    """æ··å…ƒ3Dåˆ›ä½œé¡µé¢"""
    return send_from_directory('.', 'hunyuan3d.html')

# =============== ä¸»ç¨‹åºå…¥å£ ===============
if __name__ == '__main__':
    # åˆå§‹åŒ–
    print("="*60)
    print("ğŸ¨ å›½ç”»é…è‰²ç³»ç»Ÿ + ğŸ¤– è…¾è®¯æ··å…ƒ3Dä¸“ä¸šç‰ˆï¼ˆé˜²ç©ºåŠ å¼ºç‰ˆï¼‰")
    print("="*60)
    
    # æ£€æŸ¥å¯†é’¥é…ç½®
    if TENCENT_SECRET_ID and TENCENT_SECRET_KEY:
        print("âœ… æ··å…ƒ3Då¯†é’¥å·²é…ç½®")
    else:
        print("âš ï¸ æ··å…ƒ3Då¯†é’¥æœªé…ç½®")
        print("   è¯·åœ¨ä»£ç ç¬¬68-69è¡Œè®¾ç½®")
    
    # é˜²ç©ºï¼šæ˜¾ç¤ºé…ç½®
    print("\n" + "="*60)
    print("  é˜²ç©ºé…ç½®")
    print("="*60)
    print(f"   å›¾ç‰‡å¤§å°é™åˆ¶: {MAX_IMAGE_SIZE / 1024 / 1024}MB")
    print(f"   å›¾ç‰‡å°ºå¯¸é™åˆ¶: {MAX_IMAGE_DIMENSION}px")
    print(f"   å…è®¸ç±»å‹: {ALLOWED_IMAGE_TYPES}")
    print(f"   é¢‘ç‡é™åˆ¶: {RATE_LIMIT_PER_MIN}æ¬¡/åˆ†é’Ÿ")
    print("="*60)
    
    load_models()
    load_lookup_table()
    
    print("\n" + "="*60)
    print("  æœåŠ¡çŠ¶æ€")
    print("="*60)
    print(f"   âœ… KMå­˜åœ¨æ€§åˆ¤æ–­: {KM_IMPROVED_PATH}")
    print(f"   âœ… HybridéªŒè¯: {HYBRID_MODEL_PATH}")
    print(f"   âœ… é€†å‘Transformer: {INVERSE_MODEL_PATH}")
    print(f"   å­˜åœ¨æ€§é˜ˆå€¼: {PRESENCE_THRESHOLD}  | æœ€å°‘ä¿ç•™: {MIN_RETAIN_COLORS}ç§")
    print(f"   Hybridè¯¯å·®é˜ˆå€¼: {HYBRID_ERROR_THRESHOLD}")
    print("="*60)
    print("âœ… æœåŠ¡å¯åŠ¨æˆåŠŸï¼")
    print("   å›½ç”»é…è‰²: http://localhost:5000/photo_looker.html")
    print("   AI 3Dåˆ›ä½œ: http://localhost:5000/hunyuan3d")
    print("="*60 + "\n")
    
    app.run(debug=False, port=5000, host='0.0.0.0')
